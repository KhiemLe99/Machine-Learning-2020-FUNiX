{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cnD1ecF0Mw0u"
   },
   "source": [
    "# Feature Selection and LASSO (Interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6oYlO_fMw0z"
   },
   "source": [
    "In this notebook, you will use LASSO to select features, building on a pre-implemented solver for LASSO (in sklearn, obviously). You will:\n",
    "* Run LASSO with different L1 penalties.\n",
    "* Choose best L1 penalty using a validation set.\n",
    "* Choose best L1 penalty using a validation set, with additional constraint on the size of subset.\n",
    "\n",
    "In the next exercise, you will implement your own LASSO solver, using coordinate descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRSBur8HMw02"
   },
   "source": [
    "## The usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjaAEidBMw05"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log, sqrt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBmiSN8sMw1A"
   },
   "source": [
    "## Load in house sales data\n",
    "\n",
    "Dataset is from house sales in King County, the region where the city of Seattle, WA is located. *I am so surprised.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cDLiqAIMw1D",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_data = pd.read_csv(\"../dataset/house.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyLeYGvlMw1J"
   },
   "source": [
    "## Create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSMOdbHVMw1L"
   },
   "source": [
    "As in Lab 2 (*lab-2.ipynb*), we consider features that are some transformations of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYzffDTvMw1M"
   },
   "outputs": [],
   "source": [
    "full_data['sqft_living_sqrt'] = full_data['sqft_living'].map(sqrt)\n",
    "full_data['sqft_lot_sqrt'] = full_data['sqft_lot'].map(sqrt)\n",
    "full_data['bedrooms_square'] = full_data['bedrooms'] ** 2\n",
    "\n",
    "# In the dataset, 'floors' was defined with type string, \n",
    "# so we'll convert them to float, before creating a new feature.\n",
    "full_data['floors'] = full_data['floors'].astype(float) \n",
    "full_data['floors_square'] = full_data['floors'] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z672nh0kMw1S"
   },
   "source": [
    "* Squaring bedrooms will increase the separation between not many bedrooms (e.g. 1) and lots of bedrooms (e.g. 4) since 1^2 = 1 but 4^2 = 16. Consequently this variable will mostly affect houses with many bedrooms.\n",
    "* On the other hand, taking square root of sqft_living will decrease the separation between big house and small house. The owner may not be exactly twice as happy for getting a house that is twice as big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDo2w5chMw1U"
   },
   "source": [
    "# Learn regression weights with L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-xMl9bUMw1V"
   },
   "source": [
    "Let us fit a model with all the features available, plus the features we just created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYAUT1PLMw1W"
   },
   "outputs": [],
   "source": [
    "all_features = ['bedrooms', 'bedrooms_square',\n",
    "            'bathrooms',\n",
    "            'sqft_living', 'sqft_living_sqrt',\n",
    "            'sqft_lot', 'sqft_lot_sqrt',\n",
    "            'floors', 'floors_square',\n",
    "            'waterfront', 'view', 'condition', 'grade',\n",
    "            'sqft_above',\n",
    "            'sqft_basement',\n",
    "            'yr_built', 'yr_renovated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7-592ikMw1e"
   },
   "source": [
    "Applying L1 penalty requires adding an extra parameter (`alpha=l1_penalty`) to the sklearn model `Lasso`. (Other tools may have separate implementations of LASSO). Much like L2/Ridge Regression, the features should be scaled to ensure equal attention inbetween."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlTbDC0RMw1g"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "l1_penalty=1e4\n",
    "full_features = scaler.fit_transform(full_data[all_features].values)\n",
    "full_labels = full_data['price'].values\n",
    "model = Lasso(alpha=l1_penalty).fit(full_features, full_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rH4847CcMw1m"
   },
   "source": [
    "Find what features had non-zero weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HElqPj8LMw1n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot_sqrt', 'floors_square', 'waterfront', 'view', 'condition', 'grade', 'yr_built'] 10\n"
     ]
    }
   ],
   "source": [
    "# Do you know that even numpy has built-in boolean selector?\n",
    "selected_features = [all_features[i] for i in range(len(all_features)) if model.coef_[i] != 0]\n",
    "print(selected_features, len(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0gF5MD_Mw1r"
   },
   "source": [
    "Note that a majority of the weights have been set to zero. So by setting an L1 penalty that's large enough, we are performing a subset selection. \n",
    "\n",
    "***QUIZ QUESTION***:\n",
    "According to this list of weights, which of the features have been chosen? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Zv3CFZBMw1s"
   },
   "source": [
    "# Selecting an L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oc6Vm3RfMw1t"
   },
   "source": [
    "To find a good L1 penalty, we will explore multiple values using a validation set. Let us do three way split into train, validation, and test sets:\n",
    "* Split our sales data into 2 sets: training and test (9/1)\n",
    "* Further split our training data into two sets: train, validation (5/5)\n",
    "\n",
    "Be *very* careful that you use seed = 1 to ensure you get the same answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U41_wuIcMw1v"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(full_data, train_size=0.9, test_size=0.1, random_state=1)\n",
    "train_data, valid_data = train_test_split(train_data, train_size=0.5, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7kCoG3VwMw10"
   },
   "source": [
    "Next, we write a loop that does the following:\n",
    "* For `l1_penalty` in 21 steps range between [1, 10^9] (use `np.logspace(0, 9, num=21)`.)\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty` in the parameter.\n",
    "    * Compute the RSS on VALIDATION data (here you will want to use `.predict()`) for that `l1_penalty`\n",
    "* Report which `l1_penalty` produced the lowest RSS on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handholding level: Nominally.\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_data[all_features].values)\n",
    "train_labels = train_data['price'].values\n",
    "valid_features = scaler.transform(valid_data[all_features].values)\n",
    "valid_labels = valid_data['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYf8zDJtMw11"
   },
   "outputs": [],
   "source": [
    "loss = []\n",
    "weights = []\n",
    "for l1_penalty in np.logspace(0, 9, num=21):\n",
    "    model = Lasso(alpha=l1_penalty).fit(train_features, train_labels)\n",
    "    weights.append(model.coef_)\n",
    "    predictions = model.predict(valid_features)\n",
    "    residuals = valid_labels - predictions\n",
    "    rss = sum(residuals * residuals)\n",
    "    loss.append(rss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAneHq6LMw15"
   },
   "source": [
    "*** QUIZ QUESTION. *** What was the best value for the `l1_penalty`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZ1BXWL7Mw16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 415874407867443.3\n"
     ]
    }
   ],
   "source": [
    "print(loss.index(min(loss)), min(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PcnSvUHyMw19"
   },
   "source": [
    "***QUIZ QUESTION***\n",
    "Also, using this value of L1 penalty, how many nonzero weights do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Od-dOA-IMw1-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bedrooms', 'bedrooms_square', 'bathrooms', 'sqft_living', 'sqft_living_sqrt', 'sqft_lot', 'sqft_lot_sqrt', 'floors', 'floors_square', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated'] 17\n"
     ]
    }
   ],
   "source": [
    "selected_features = [all_features[i] for i in range(len(all_features)) if weights[6][i] != 0]\n",
    "print(selected_features, len(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zNo9ywLMw2F"
   },
   "source": [
    "# Limit the number of nonzero weights\n",
    "\n",
    "What if we absolutely wanted to limit ourselves to, say, 5 features? This may be important if we want to derive \"a rule of thumb\" --- an interpretable model that has only a few features in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLijk-3UMw2G"
   },
   "source": [
    "In this section, you are going to implement a simple, two phase procedure to achive this goal:\n",
    "1. Explore a large range of `l1_penalty` values to find a narrow region of `l1_penalty` values where models are likely to have the desired number of non-zero weights.\n",
    "2. Further explore the narrow region you found to find a good value for `l1_penalty` that achieves the desired sparsity.  Here, we will again use a validation set to choose the best value for `l1_penalty`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOwQHIzMMw2H"
   },
   "outputs": [],
   "source": [
    "max_nonzeros = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMv9UnluMw2N"
   },
   "source": [
    "## Exploring the larger range of values to find a narrow range with the desired sparsity\n",
    "\n",
    "Let's define a wide range of possible `l1_penalty_values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vEaNt8VHMw2O"
   },
   "outputs": [],
   "source": [
    "l1_penalty_values = np.logspace(3, 5, num=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mxrj8371Mw2S"
   },
   "source": [
    "Now, implement a loop that search through this space of possible `l1_penalty` values:\n",
    "\n",
    "* For `l1_penalty` in `np.logspace(3, 5, num=21)`:\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty` in the parameter.\n",
    "    * Extract the weights of the model and count the number of nonzeros. Save the number of nonzeros to a list.\n",
    "        * *Hint: `model.coef_` gives you the coefficients/parameters you learned (barring intercept) in the form of numpy array. You can then use array\\[condition\\] for the list of values passing the condition. Or just use the builtin `np.count_nonzero()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kFzVsaiMw2U"
   },
   "outputs": [],
   "source": [
    "loss = []\n",
    "count_nonzero = []\n",
    "for l1_penalty in l1_penalty_values:\n",
    "    model = Lasso(alpha=l1_penalty).fit(train_features, train_labels)\n",
    "    selected_features = [all_features[i] for i in range(len(all_features)) if model.coef_[i] != 0]\n",
    "    count_nonzero.append(len(selected_features))\n",
    "    predictions = model.predict(valid_features)\n",
    "    residuals = valid_labels - predictions\n",
    "    rss = sum(residuals * residuals)\n",
    "    loss.append(rss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucgWXpj9Mw2Z"
   },
   "source": [
    "Out of this large range, we want to find the two ends of our desired narrow range of `l1_penalty`.  At one end, we will have `l1_penalty` values that have too few non-zeros, and at the other end, we will have an `l1_penalty` that has too many non-zeros.  \n",
    "\n",
    "More formally, find:\n",
    "* The smallest `l1_penalty` that has non-zeros equal `max_nonzeros` (if we pick a penalty smaller than this value, we will definitely have too many non-zero weights)\n",
    "    * Store this value in the variable `l1_penalty_min` (we will use it later)\n",
    "* The biggest `l1_penalty` that has non-zeros equal `max_nonzeros` (if we pick a penalty larger than this value, we will definitely have too few non-zero weights)\n",
    "    * Store this value in the variable `l1_penalty_max` (we will use it later)\n",
    "\n",
    "\n",
    "*Hint: there are many ways to do this, e.g.:*\n",
    "* Programmatically within the loop above\n",
    "* Creating a list with the number of non-zeros for each value of `l1_penalty` and inspecting it to find the appropriate boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBpwiXEVMw2b"
   },
   "outputs": [],
   "source": [
    "l1_penalty_min = l1_penalty_values[count_nonzero.index(max_nonzeros)]\n",
    "l1_penalty_max = l1_penalty_values[::-1][count_nonzero[::-1].index(max_nonzeros)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25118.864315095823, 50118.72336272725)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_penalty_min, l1_penalty_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-S2jtklaMw2g"
   },
   "source": [
    "***QUIZ QUESTION.*** What values did you find for `l1_penalty_min` and `l1_penalty_max`, respectively? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-T_xUbJ5Mw2i"
   },
   "source": [
    "## Exploring the narrow range of values to find the solution with the right number of non-zeros that has lowest RSS on the validation set \n",
    "\n",
    "We will now explore the narrow region of `l1_penalty` values we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mEICQwrAMw2k"
   },
   "outputs": [],
   "source": [
    "l1_penalty_values = np.linspace(l1_penalty_min, l1_penalty_max,  20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IVkWedu9Mw2p"
   },
   "source": [
    "* For `l1_penalty` in `np.linspace(l1_penalty_min,l1_penalty_max,20)`:\n",
    "    * Fit a regression model with a given `l1_penalty` on TRAIN data. Specify `alpha=l1_penalty`.\n",
    "    * Measure the RSS of the learned model on the VALIDATION set\n",
    "\n",
    "Find the model that the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKsCKTP3Mw2q"
   },
   "outputs": [],
   "source": [
    "loss = []\n",
    "count_nonzero = []\n",
    "weights = []\n",
    "l1_penalty_values = np.linspace(l1_penalty_min,l1_penalty_max,20)\n",
    "for l1_penalty in l1_penalty_values:\n",
    "    model = Lasso(alpha=l1_penalty).fit(train_features, train_labels)\n",
    "    weights.append(model.coef_)\n",
    "    selected_features = [all_features[i] for i in range(len(all_features)) if model.coef_[i] != 0]\n",
    "    count_nonzero.append(len(selected_features))\n",
    "    predictions = model.predict(valid_features)\n",
    "    residuals = valid_labels - predictions\n",
    "    rss = sum(residuals * residuals)\n",
    "    loss.append(rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25118.864315095823"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_penalty_values[loss.index(min(loss))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sqft_living', 'waterfront', 'view', 'grade', 'yr_built'] 5\n"
     ]
    }
   ],
   "source": [
    "selected_features = [all_features[i] for i in range(len(all_features)) if weights[loss.index(min(loss))][i] != 0]\n",
    "print(selected_features, len(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4vAuKXfMw2u"
   },
   "source": [
    "***QUIZ QUESTIONS***\n",
    "1. What value of `l1_penalty` in our narrow range has the lowest RSS on the VALIDATION set and has sparsity *equal* to `max_nonzeros`?\n",
    "2. What features in this model have non-zero coefficients?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "lab-5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
